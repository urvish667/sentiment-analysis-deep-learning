{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657ecaf9-de33-4cf0-9c8f-7b7e63636f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Urvish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Urvish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random as rnd\n",
    "torch.manual_seed(42);\n",
    "\n",
    "from utils import Layer, load_tweets, process_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fca7db0-3b38-4e68-9e5d-a4e8b95d887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(5.0)\n",
    "\n",
    "display(a)\n",
    "\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa36ac6b-b1ab-4f84-8df9-060fdd74bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1353cba-6c6b-40a6-8c55-4904ba8f257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0 is 25.0\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "print(f\"f(a) for a={a} is {f(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb519919-512e-43f4-b3fa-4f7c47e8d800",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a9631d-f12d-4074-b08e-f01dbba78164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load positive and negative tweets\n",
    "all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "# View the total number of positive and negative tweets.\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "# Split positive set into validation and training\n",
    "val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a303d1-4325-4e2d-96b6-24441efd3a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tweet at position 0:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "processed tweet at postion 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the tweets\n",
    "print('original tweet at position 0:')\n",
    "print(train_x[0])\n",
    "\n",
    "print('processed tweet at postion 0:')\n",
    "process_tweet(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb110407-6f3f-4abd-9f24-46af589b065d",
   "metadata": {},
   "source": [
    "### Building vocabulary\n",
    "\n",
    "- `__PAD__`: padding\n",
    "- `__</e>__`: end of line\n",
    "- `__UNK__`: a token to represent unknown word that is not the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf572ca-3a31-4273-8b70-e58a373f4632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'followfriday': 3,\n",
       " 'top': 4,\n",
       " 'engag': 5,\n",
       " 'member': 6,\n",
       " 'commun': 7,\n",
       " 'week': 8,\n",
       " ':)': 9,\n",
       " 'hey': 10,\n",
       " 'jame': 11,\n",
       " 'odd': 12,\n",
       " ':/': 13,\n",
       " 'pleas': 14,\n",
       " 'call': 15,\n",
       " 'contact': 16,\n",
       " 'centr': 17,\n",
       " '02392441234': 18,\n",
       " 'abl': 19,\n",
       " 'assist': 20,\n",
       " 'mani': 21,\n",
       " 'thank': 22,\n",
       " 'listen': 23,\n",
       " 'last': 24,\n",
       " 'night': 25,\n",
       " 'bleed': 26,\n",
       " 'amaz': 27,\n",
       " 'track': 28,\n",
       " 'scotland': 29,\n",
       " 'congrat': 30,\n",
       " 'yeaaah': 31,\n",
       " 'yipppi': 32,\n",
       " 'accnt': 33,\n",
       " 'verifi': 34,\n",
       " 'rqst': 35,\n",
       " 'succeed': 36,\n",
       " 'got': 37,\n",
       " 'blue': 38,\n",
       " 'tick': 39,\n",
       " 'mark': 40,\n",
       " 'fb': 41,\n",
       " 'profil': 42,\n",
       " '15': 43,\n",
       " 'day': 44,\n",
       " 'one': 45,\n",
       " 'irresist': 46,\n",
       " 'flipkartfashionfriday': 47,\n",
       " 'like': 48,\n",
       " 'keep': 49,\n",
       " 'love': 50,\n",
       " 'custom': 51,\n",
       " 'wait': 52,\n",
       " 'long': 53,\n",
       " 'hope': 54,\n",
       " 'enjoy': 55,\n",
       " 'happi': 56,\n",
       " 'friday': 57,\n",
       " 'lwwf': 58,\n",
       " 'second': 59,\n",
       " 'thought': 60,\n",
       " '‚Äô': 61,\n",
       " 'enough': 62,\n",
       " 'time': 63,\n",
       " 'dd': 64,\n",
       " 'new': 65,\n",
       " 'short': 66,\n",
       " 'enter': 67,\n",
       " 'system': 68,\n",
       " 'sheep': 69,\n",
       " 'must': 70,\n",
       " 'buy': 71,\n",
       " 'jgh': 72,\n",
       " 'go': 73,\n",
       " 'bayan': 74,\n",
       " ':d': 75,\n",
       " 'bye': 76,\n",
       " 'act': 77,\n",
       " 'mischiev': 78,\n",
       " 'etl': 79,\n",
       " 'layer': 80,\n",
       " 'in-hous': 81,\n",
       " 'wareh': 82,\n",
       " 'app': 83,\n",
       " 'katamari': 84,\n",
       " 'well': 85,\n",
       " '‚Ä¶': 86,\n",
       " 'name': 87,\n",
       " 'impli': 88,\n",
       " ':p': 89,\n",
       " 'influenc': 90,\n",
       " 'big': 91,\n",
       " '...': 92,\n",
       " 'juici': 93,\n",
       " 'selfi': 94,\n",
       " 'follow': 95,\n",
       " 'perfect': 96,\n",
       " 'alreadi': 97,\n",
       " 'know': 98,\n",
       " \"what'\": 99,\n",
       " 'great': 100,\n",
       " 'opportun': 101,\n",
       " 'junior': 102,\n",
       " 'triathlet': 103,\n",
       " 'age': 104,\n",
       " '12': 105,\n",
       " '13': 106,\n",
       " 'gatorad': 107,\n",
       " 'seri': 108,\n",
       " 'get': 109,\n",
       " 'entri': 110,\n",
       " 'lay': 111,\n",
       " 'greet': 112,\n",
       " 'card': 113,\n",
       " 'rang': 114,\n",
       " 'print': 115,\n",
       " 'today': 116,\n",
       " 'job': 117,\n",
       " ':-)': 118,\n",
       " \"friend'\": 119,\n",
       " 'lunch': 120,\n",
       " 'yummm': 121,\n",
       " 'nostalgia': 122,\n",
       " 'tb': 123,\n",
       " 'ku': 124,\n",
       " 'id': 125,\n",
       " 'conflict': 126,\n",
       " 'help': 127,\n",
       " \"here'\": 128,\n",
       " 'screenshot': 129,\n",
       " 'work': 130,\n",
       " 'hi': 131,\n",
       " 'liv': 132,\n",
       " 'hello': 133,\n",
       " 'need': 134,\n",
       " 'someth': 135,\n",
       " 'u': 136,\n",
       " 'fm': 137,\n",
       " 'twitter': 138,\n",
       " '‚Äî': 139,\n",
       " 'sure': 140,\n",
       " 'thing': 141,\n",
       " 'dm': 142,\n",
       " 'x': 143,\n",
       " \"i'v\": 144,\n",
       " 'heard': 145,\n",
       " 'four': 146,\n",
       " 'season': 147,\n",
       " 'pretti': 148,\n",
       " 'dope': 149,\n",
       " 'penthous': 150,\n",
       " 'obv': 151,\n",
       " 'gobigorgohom': 152,\n",
       " 'fun': 153,\n",
       " \"y'all\": 154,\n",
       " 'yeah': 155,\n",
       " 'suppos': 156,\n",
       " 'lol': 157,\n",
       " 'chat': 158,\n",
       " 'bit': 159,\n",
       " 'youth': 160,\n",
       " 'üíÖüèΩ': 161,\n",
       " 'üíã': 162,\n",
       " 'seen': 163,\n",
       " 'year': 164,\n",
       " 'rest': 165,\n",
       " 'goe': 166,\n",
       " 'quickli': 167,\n",
       " 'bed': 168,\n",
       " 'music': 169,\n",
       " 'fix': 170,\n",
       " 'dream': 171,\n",
       " 'spiritu': 172,\n",
       " 'ritual': 173,\n",
       " 'festiv': 174,\n",
       " 'n√©pal': 175,\n",
       " 'begin': 176,\n",
       " 'line-up': 177,\n",
       " 'left': 178,\n",
       " 'see': 179,\n",
       " 'sarah': 180,\n",
       " 'send': 181,\n",
       " 'us': 182,\n",
       " 'email': 183,\n",
       " 'bitsy@bitdefender.com': 184,\n",
       " \"we'll\": 185,\n",
       " 'asap': 186,\n",
       " 'kik': 187,\n",
       " 'hatessuc': 188,\n",
       " '32429': 189,\n",
       " 'kikm': 190,\n",
       " 'lgbt': 191,\n",
       " 'tinder': 192,\n",
       " 'nsfw': 193,\n",
       " 'akua': 194,\n",
       " 'cumshot': 195,\n",
       " 'come': 196,\n",
       " 'hous': 197,\n",
       " 'nsn_supplement': 198,\n",
       " 'effect': 199,\n",
       " 'press': 200,\n",
       " 'releas': 201,\n",
       " 'distribut': 202,\n",
       " 'result': 203,\n",
       " 'link': 204,\n",
       " 'remov': 205,\n",
       " 'pressreleas': 206,\n",
       " 'newsdistribut': 207,\n",
       " 'bam': 208,\n",
       " 'bestfriend': 209,\n",
       " 'lot': 210,\n",
       " 'warsaw': 211,\n",
       " '<3': 212,\n",
       " 'x46': 213,\n",
       " 'everyon': 214,\n",
       " 'watch': 215,\n",
       " 'documentari': 216,\n",
       " 'earthl': 217,\n",
       " 'youtub': 218,\n",
       " 'support': 219,\n",
       " 'buuut': 220,\n",
       " 'oh': 221,\n",
       " 'look': 222,\n",
       " 'forward': 223,\n",
       " 'visit': 224,\n",
       " 'next': 225,\n",
       " 'letsgetmessi': 226,\n",
       " 'jo': 227,\n",
       " 'make': 228,\n",
       " 'feel': 229,\n",
       " 'better': 230,\n",
       " 'never': 231,\n",
       " 'anyon': 232,\n",
       " 'kpop': 233,\n",
       " 'flesh': 234,\n",
       " 'good': 235,\n",
       " 'girl': 236,\n",
       " 'best': 237,\n",
       " 'wish': 238,\n",
       " 'reason': 239,\n",
       " 'epic': 240,\n",
       " 'soundtrack': 241,\n",
       " 'shout': 242,\n",
       " 'ad': 243,\n",
       " 'video': 244,\n",
       " 'playlist': 245,\n",
       " 'would': 246,\n",
       " 'dear': 247,\n",
       " 'jordan': 248,\n",
       " 'okay': 249,\n",
       " 'fake': 250,\n",
       " 'gameplay': 251,\n",
       " ';)': 252,\n",
       " 'haha': 253,\n",
       " 'im': 254,\n",
       " 'kid': 255,\n",
       " 'stuff': 256,\n",
       " 'exactli': 257,\n",
       " 'product': 258,\n",
       " 'line': 259,\n",
       " 'etsi': 260,\n",
       " 'shop': 261,\n",
       " 'check': 262,\n",
       " 'vacat': 263,\n",
       " 'recharg': 264,\n",
       " 'normal': 265,\n",
       " 'charger': 266,\n",
       " 'asleep': 267,\n",
       " 'talk': 268,\n",
       " 'sooo': 269,\n",
       " 'someon': 270,\n",
       " 'text': 271,\n",
       " 'ye': 272,\n",
       " 'bet': 273,\n",
       " \"he'll\": 274,\n",
       " 'fit': 275,\n",
       " 'hear': 276,\n",
       " 'speech': 277,\n",
       " 'piti': 278,\n",
       " 'green': 279,\n",
       " 'garden': 280,\n",
       " 'midnight': 281,\n",
       " 'sun': 282,\n",
       " 'beauti': 283,\n",
       " 'canal': 284,\n",
       " 'dasvidaniya': 285,\n",
       " 'till': 286,\n",
       " 'scout': 287,\n",
       " 'sg': 288,\n",
       " 'futur': 289,\n",
       " 'wlan': 290,\n",
       " 'pro': 291,\n",
       " 'confer': 292,\n",
       " 'asia': 293,\n",
       " 'chang': 294,\n",
       " 'lollipop': 295,\n",
       " 'üç≠': 296,\n",
       " 'nez': 297,\n",
       " 'agnezmo': 298,\n",
       " 'oley': 299,\n",
       " 'mama': 300,\n",
       " 'stand': 301,\n",
       " 'stronger': 302,\n",
       " 'god': 303,\n",
       " 'misti': 304,\n",
       " 'babi': 305,\n",
       " 'cute': 306,\n",
       " 'woohoo': 307,\n",
       " \"can't\": 308,\n",
       " 'sign': 309,\n",
       " 'yet': 310,\n",
       " 'still': 311,\n",
       " 'think': 312,\n",
       " 'mka': 313,\n",
       " 'liam': 314,\n",
       " 'access': 315,\n",
       " 'welcom': 316,\n",
       " 'stat': 317,\n",
       " 'arriv': 318,\n",
       " '1': 319,\n",
       " 'unfollow': 320,\n",
       " 'via': 321,\n",
       " 'surpris': 322,\n",
       " 'figur': 323,\n",
       " 'happybirthdayemilybett': 324,\n",
       " 'sweet': 325,\n",
       " 'talent': 326,\n",
       " '2': 327,\n",
       " 'plan': 328,\n",
       " 'drain': 329,\n",
       " 'gotta': 330,\n",
       " 'timezon': 331,\n",
       " 'parent': 332,\n",
       " 'proud': 333,\n",
       " 'least': 334,\n",
       " 'mayb': 335,\n",
       " 'sometim': 336,\n",
       " 'grade': 337,\n",
       " 'al': 338,\n",
       " 'grand': 339,\n",
       " 'manila_bro': 340,\n",
       " 'chosen': 341,\n",
       " 'let': 342,\n",
       " 'around': 343,\n",
       " '..': 344,\n",
       " 'side': 345,\n",
       " 'world': 346,\n",
       " 'eh': 347,\n",
       " 'take': 348,\n",
       " 'care': 349,\n",
       " 'final': 350,\n",
       " 'fuck': 351,\n",
       " 'weekend': 352,\n",
       " 'real': 353,\n",
       " 'x45': 354,\n",
       " 'join': 355,\n",
       " 'hushedcallwithfraydo': 356,\n",
       " 'gift': 357,\n",
       " 'yeahhh': 358,\n",
       " 'hushedpinwithsammi': 359,\n",
       " 'event': 360,\n",
       " 'might': 361,\n",
       " 'luv': 362,\n",
       " 'realli': 363,\n",
       " 'appreci': 364,\n",
       " 'share': 365,\n",
       " 'wow': 366,\n",
       " 'tom': 367,\n",
       " 'gym': 368,\n",
       " 'monday': 369,\n",
       " 'invit': 370,\n",
       " 'scope': 371,\n",
       " 'friend': 372,\n",
       " 'nude': 373,\n",
       " 'sleep': 374,\n",
       " 'birthday': 375,\n",
       " 'want': 376,\n",
       " 't-shirt': 377,\n",
       " 'cool': 378,\n",
       " 'haw': 379,\n",
       " 'phela': 380,\n",
       " 'mom': 381,\n",
       " 'obvious': 382,\n",
       " 'princ': 383,\n",
       " 'charm': 384,\n",
       " 'stage': 385,\n",
       " 'luck': 386,\n",
       " 'tyler': 387,\n",
       " 'hipster': 388,\n",
       " 'glass': 389,\n",
       " 'marti': 390,\n",
       " 'glad': 391,\n",
       " 'done': 392,\n",
       " 'afternoon': 393,\n",
       " 'read': 394,\n",
       " 'kahfi': 395,\n",
       " 'finish': 396,\n",
       " 'ohmyg': 397,\n",
       " 'yaya': 398,\n",
       " 'dub': 399,\n",
       " 'stalk': 400,\n",
       " 'ig': 401,\n",
       " 'gondooo': 402,\n",
       " 'moo': 403,\n",
       " 'tologooo': 404,\n",
       " 'becom': 405,\n",
       " 'detail': 406,\n",
       " 'zzz': 407,\n",
       " 'xx': 408,\n",
       " 'physiotherapi': 409,\n",
       " 'hashtag': 410,\n",
       " 'üí™': 411,\n",
       " 'monica': 412,\n",
       " 'miss': 413,\n",
       " 'sound': 414,\n",
       " 'morn': 415,\n",
       " \"that'\": 416,\n",
       " 'x43': 417,\n",
       " 'definit': 418,\n",
       " 'tri': 419,\n",
       " 'tonight': 420,\n",
       " 'took': 421,\n",
       " 'advic': 422,\n",
       " 'treviso': 423,\n",
       " 'concert': 424,\n",
       " 'citi': 425,\n",
       " 'countri': 426,\n",
       " \"i'll\": 427,\n",
       " 'start': 428,\n",
       " 'fine': 429,\n",
       " 'gorgeou': 430,\n",
       " 'xo': 431,\n",
       " 'oven': 432,\n",
       " 'roast': 433,\n",
       " 'garlic': 434,\n",
       " 'oliv': 435,\n",
       " 'oil': 436,\n",
       " 'dri': 437,\n",
       " 'tomato': 438,\n",
       " 'basil': 439,\n",
       " 'centuri': 440,\n",
       " 'tuna': 441,\n",
       " 'right': 442,\n",
       " 'back': 443,\n",
       " 'atchya': 444,\n",
       " 'even': 445,\n",
       " 'almost': 446,\n",
       " 'chanc': 447,\n",
       " 'cheer': 448,\n",
       " 'po': 449,\n",
       " 'ice': 450,\n",
       " 'cream': 451,\n",
       " 'agre': 452,\n",
       " '100': 453,\n",
       " 'heheheh': 454,\n",
       " 'that': 455,\n",
       " 'point': 456,\n",
       " 'stay': 457,\n",
       " 'home': 458,\n",
       " 'soon': 459,\n",
       " 'promis': 460,\n",
       " 'web': 461,\n",
       " 'whatsapp': 462,\n",
       " 'volta': 463,\n",
       " 'funcionar': 464,\n",
       " 'com': 465,\n",
       " 'iphon': 466,\n",
       " 'jailbroken': 467,\n",
       " 'later': 468,\n",
       " '34': 469,\n",
       " 'min': 470,\n",
       " 'leia': 471,\n",
       " 'appear': 472,\n",
       " 'hologram': 473,\n",
       " 'r2d2': 474,\n",
       " 'w': 475,\n",
       " 'messag': 476,\n",
       " 'obi': 477,\n",
       " 'wan': 478,\n",
       " 'sit': 479,\n",
       " 'luke': 480,\n",
       " 'inter': 481,\n",
       " '3': 482,\n",
       " 'ucl': 483,\n",
       " 'arsen': 484,\n",
       " 'small': 485,\n",
       " 'team': 486,\n",
       " 'pass': 487,\n",
       " 'üöÇ': 488,\n",
       " 'dewsburi': 489,\n",
       " 'railway': 490,\n",
       " 'station': 491,\n",
       " 'dew': 492,\n",
       " 'west': 493,\n",
       " 'yorkshir': 494,\n",
       " '430': 495,\n",
       " 'smh': 496,\n",
       " '9:25': 497,\n",
       " 'live': 498,\n",
       " 'strang': 499,\n",
       " 'imagin': 500,\n",
       " 'megan': 501,\n",
       " 'masaantoday': 502,\n",
       " 'a4': 503,\n",
       " 'shweta': 504,\n",
       " 'tripathi': 505,\n",
       " '5': 506,\n",
       " '20': 507,\n",
       " 'kurta': 508,\n",
       " 'half': 509,\n",
       " 'number': 510,\n",
       " 'wsalelov': 511,\n",
       " 'ah': 512,\n",
       " 'larri': 513,\n",
       " 'anyway': 514,\n",
       " 'kinda': 515,\n",
       " 'goood': 516,\n",
       " 'life': 517,\n",
       " 'enn': 518,\n",
       " 'could': 519,\n",
       " 'warmup': 520,\n",
       " '15th': 521,\n",
       " 'bath': 522,\n",
       " 'dum': 523,\n",
       " 'andar': 524,\n",
       " 'ram': 525,\n",
       " 'sampath': 526,\n",
       " 'sona': 527,\n",
       " 'mohapatra': 528,\n",
       " 'samantha': 529,\n",
       " 'edward': 530,\n",
       " 'mein': 531,\n",
       " 'tulan': 532,\n",
       " 'razi': 533,\n",
       " 'wah': 534,\n",
       " 'josh': 535,\n",
       " 'alway': 536,\n",
       " 'smile': 537,\n",
       " 'pictur': 538,\n",
       " '16.20': 539,\n",
       " 'giveitup': 540,\n",
       " 'given': 541,\n",
       " 'ga': 542,\n",
       " 'subsidi': 543,\n",
       " 'initi': 544,\n",
       " 'propos': 545,\n",
       " 'delight': 546,\n",
       " 'yesterday': 547,\n",
       " 'x42': 548,\n",
       " 'lmaoo': 549,\n",
       " 'song': 550,\n",
       " 'ever': 551,\n",
       " 'shall': 552,\n",
       " 'littl': 553,\n",
       " 'throwback': 554,\n",
       " 'outli': 555,\n",
       " 'island': 556,\n",
       " 'cheung': 557,\n",
       " 'chau': 558,\n",
       " 'mui': 559,\n",
       " 'wo': 560,\n",
       " 'total': 561,\n",
       " 'differ': 562,\n",
       " 'kfckitchentour': 563,\n",
       " 'kitchen': 564,\n",
       " 'clean': 565,\n",
       " \"i'm\": 566,\n",
       " 'cusp': 567,\n",
       " 'test': 568,\n",
       " 'water': 569,\n",
       " 'reward': 570,\n",
       " 'arummzz': 571,\n",
       " \"let'\": 572,\n",
       " 'drive': 573,\n",
       " 'travel': 574,\n",
       " 'yogyakarta': 575,\n",
       " 'jeep': 576,\n",
       " 'indonesia': 577,\n",
       " 'instamood': 578,\n",
       " 'wanna': 579,\n",
       " 'skype': 580,\n",
       " 'may': 581,\n",
       " 'nice': 582,\n",
       " 'friendli': 583,\n",
       " 'pretend': 584,\n",
       " 'film': 585,\n",
       " 'congratul': 586,\n",
       " 'winner': 587,\n",
       " 'cheesydelight': 588,\n",
       " 'contest': 589,\n",
       " 'address': 590,\n",
       " 'guy': 591,\n",
       " 'market': 592,\n",
       " '24/7': 593,\n",
       " '14': 594,\n",
       " 'hour': 595,\n",
       " 'leav': 596,\n",
       " 'without': 597,\n",
       " 'delay': 598,\n",
       " 'actual': 599,\n",
       " 'easi': 600,\n",
       " 'guess': 601,\n",
       " 'train': 602,\n",
       " 'wd': 603,\n",
       " 'shift': 604,\n",
       " 'engin': 605,\n",
       " 'etc': 606,\n",
       " 'sunburn': 607,\n",
       " 'peel': 608,\n",
       " 'blog': 609,\n",
       " 'huge': 610,\n",
       " 'warm': 611,\n",
       " '‚òÜ': 612,\n",
       " 'complet': 613,\n",
       " 'triangl': 614,\n",
       " 'northern': 615,\n",
       " 'ireland': 616,\n",
       " 'sight': 617,\n",
       " 'smthng': 618,\n",
       " 'fr': 619,\n",
       " 'hug': 620,\n",
       " 'xoxo': 621,\n",
       " 'uu': 622,\n",
       " 'jaann': 623,\n",
       " 'topnewfollow': 624,\n",
       " 'connect': 625,\n",
       " 'wonder': 626,\n",
       " 'made': 627,\n",
       " 'fluffi': 628,\n",
       " 'insid': 629,\n",
       " 'pirouett': 630,\n",
       " 'moos': 631,\n",
       " 'trip': 632,\n",
       " 'philli': 633,\n",
       " 'decemb': 634,\n",
       " \"i'd\": 635,\n",
       " 'dude': 636,\n",
       " 'x41': 637,\n",
       " 'question': 638,\n",
       " 'flaw': 639,\n",
       " 'pain': 640,\n",
       " 'negat': 641,\n",
       " 'strength': 642,\n",
       " 'went': 643,\n",
       " 'solo': 644,\n",
       " 'move': 645,\n",
       " 'fav': 646,\n",
       " 'nirvana': 647,\n",
       " 'smell': 648,\n",
       " 'teen': 649,\n",
       " 'spirit': 650,\n",
       " 'rip': 651,\n",
       " 'ami': 652,\n",
       " 'winehous': 653,\n",
       " 'coupl': 654,\n",
       " 'tomhiddleston': 655,\n",
       " 'elizabetholsen': 656,\n",
       " 'yaytheylookgreat': 657,\n",
       " 'goodnight': 658,\n",
       " 'vid': 659,\n",
       " 'wake': 660,\n",
       " 'gonna': 661,\n",
       " 'shoot': 662,\n",
       " 'itti': 663,\n",
       " 'bitti': 664,\n",
       " 'teeni': 665,\n",
       " 'bikini': 666,\n",
       " 'much': 667,\n",
       " '4th': 668,\n",
       " 'togeth': 669,\n",
       " 'end': 670,\n",
       " 'xfile': 671,\n",
       " 'content': 672,\n",
       " 'rain': 673,\n",
       " 'fabul': 674,\n",
       " 'fantast': 675,\n",
       " '‚ô°': 676,\n",
       " 'jb': 677,\n",
       " 'forev': 678,\n",
       " 'belieb': 679,\n",
       " 'nighti': 680,\n",
       " 'bug': 681,\n",
       " 'bite': 682,\n",
       " 'bracelet': 683,\n",
       " 'idea': 684,\n",
       " 'foundri': 685,\n",
       " 'game': 686,\n",
       " 'sens': 687,\n",
       " 'pic': 688,\n",
       " 'ef': 689,\n",
       " 'phone': 690,\n",
       " 'woot': 691,\n",
       " 'derek': 692,\n",
       " 'use': 693,\n",
       " 'parkshar': 694,\n",
       " 'gloucestershir': 695,\n",
       " 'aaaahhh': 696,\n",
       " 'man': 697,\n",
       " 'traffic': 698,\n",
       " 'stress': 699,\n",
       " 'reliev': 700,\n",
       " \"how'r\": 701,\n",
       " 'arbeloa': 702,\n",
       " 'turn': 703,\n",
       " '17': 704,\n",
       " 'omg': 705,\n",
       " 'say': 706,\n",
       " 'europ': 707,\n",
       " 'rise': 708,\n",
       " 'find': 709,\n",
       " 'hard': 710,\n",
       " 'believ': 711,\n",
       " 'uncount': 712,\n",
       " 'coz': 713,\n",
       " 'unlimit': 714,\n",
       " 'cours': 715,\n",
       " 'teamposit': 716,\n",
       " 'aldub': 717,\n",
       " '‚òï': 718,\n",
       " 'rita': 719,\n",
       " 'info': 720,\n",
       " \"we'd\": 721,\n",
       " 'way': 722,\n",
       " 'boy': 723,\n",
       " 'x40': 724,\n",
       " 'true': 725,\n",
       " 'sethi': 726,\n",
       " 'high': 727,\n",
       " 'exe': 728,\n",
       " 'skeem': 729,\n",
       " 'saam': 730,\n",
       " 'peopl': 731,\n",
       " 'polit': 732,\n",
       " 'izzat': 733,\n",
       " 'wese': 734,\n",
       " 'trust': 735,\n",
       " 'khawateen': 736,\n",
       " 'k': 737,\n",
       " 'sath': 738,\n",
       " 'mana': 739,\n",
       " 'kar': 740,\n",
       " 'deya': 741,\n",
       " 'sort': 742,\n",
       " 'smart': 743,\n",
       " 'hair': 744,\n",
       " 'tbh': 745,\n",
       " 'jacob': 746,\n",
       " 'g': 747,\n",
       " 'upgrad': 748,\n",
       " 'tee': 749,\n",
       " 'famili': 750,\n",
       " 'person': 751,\n",
       " 'two': 752,\n",
       " 'convers': 753,\n",
       " 'onlin': 754,\n",
       " 'mclaren': 755,\n",
       " 'fridayfeel': 756,\n",
       " 'tgif': 757,\n",
       " 'squar': 758,\n",
       " 'enix': 759,\n",
       " 'bissmillah': 760,\n",
       " 'ya': 761,\n",
       " 'allah': 762,\n",
       " \"we'r\": 763,\n",
       " 'socent': 764,\n",
       " 'startup': 765,\n",
       " 'drop': 766,\n",
       " 'your': 767,\n",
       " 'arnd': 768,\n",
       " 'town': 769,\n",
       " 'basic': 770,\n",
       " 'piss': 771,\n",
       " 'cup': 772,\n",
       " 'also': 773,\n",
       " 'terribl': 774,\n",
       " 'complic': 775,\n",
       " 'discuss': 776,\n",
       " 'snapchat': 777,\n",
       " 'lynettelow': 778,\n",
       " 'kikmenow': 779,\n",
       " 'snapm': 780,\n",
       " 'hot': 781,\n",
       " 'amazon': 782,\n",
       " 'kikmeguy': 783,\n",
       " 'defin': 784,\n",
       " 'grow': 785,\n",
       " 'sport': 786,\n",
       " 'rt': 787,\n",
       " 'rakyat': 788,\n",
       " 'write': 789,\n",
       " 'sinc': 790,\n",
       " 'mention': 791,\n",
       " 'fli': 792,\n",
       " 'fish': 793,\n",
       " 'promot': 794,\n",
       " 'post': 795,\n",
       " 'cyber': 796,\n",
       " 'ourdaughtersourprid': 797,\n",
       " 'mypapamyprid': 798,\n",
       " 'papa': 799,\n",
       " 'coach': 800,\n",
       " 'posit': 801,\n",
       " 'kha': 802,\n",
       " 'atleast': 803,\n",
       " 'x39': 804,\n",
       " 'mango': 805,\n",
       " \"lassi'\": 806,\n",
       " \"monty'\": 807,\n",
       " 'marvel': 808,\n",
       " 'though': 809,\n",
       " 'suspect': 810,\n",
       " 'meant': 811,\n",
       " '24': 812,\n",
       " 'hr': 813,\n",
       " 'touch': 814,\n",
       " 'kepler': 815,\n",
       " '452b': 816,\n",
       " 'chalna': 817,\n",
       " 'hai': 818,\n",
       " 'thankyou': 819,\n",
       " 'hazel': 820,\n",
       " 'food': 821,\n",
       " 'brooklyn': 822,\n",
       " 'pta': 823,\n",
       " 'awak': 824,\n",
       " 'okayi': 825,\n",
       " 'awww': 826,\n",
       " 'ha': 827,\n",
       " 'doc': 828,\n",
       " 'splendid': 829,\n",
       " 'spam': 830,\n",
       " 'folder': 831,\n",
       " 'amount': 832,\n",
       " 'nigeria': 833,\n",
       " 'claim': 834,\n",
       " 'rted': 835,\n",
       " 'leg': 836,\n",
       " 'hurt': 837,\n",
       " 'bad': 838,\n",
       " 'mine': 839,\n",
       " 'saturday': 840,\n",
       " 'thaaank': 841,\n",
       " 'puhon': 842,\n",
       " 'happinesss': 843,\n",
       " 'tnc': 844,\n",
       " 'prior': 845,\n",
       " 'notif': 846,\n",
       " 'fat': 847,\n",
       " 'co': 848,\n",
       " 'probabl': 849,\n",
       " 'ate': 850,\n",
       " 'yuna': 851,\n",
       " 'tamesid': 852,\n",
       " '¬¥': 853,\n",
       " 'googl': 854,\n",
       " 'account': 855,\n",
       " 'scouser': 856,\n",
       " 'everyth': 857,\n",
       " 'zoe': 858,\n",
       " 'mate': 859,\n",
       " 'liter': 860,\n",
       " \"they'r\": 861,\n",
       " 'samee': 862,\n",
       " 'edgar': 863,\n",
       " 'updat': 864,\n",
       " 'log': 865,\n",
       " 'bring': 866,\n",
       " 'abe': 867,\n",
       " 'meet': 868,\n",
       " 'x38': 869,\n",
       " 'sigh': 870,\n",
       " 'dreamili': 871,\n",
       " 'pout': 872,\n",
       " 'eye': 873,\n",
       " 'quacketyquack': 874,\n",
       " 'funni': 875,\n",
       " 'happen': 876,\n",
       " 'phil': 877,\n",
       " 'em': 878,\n",
       " 'del': 879,\n",
       " 'rodder': 880,\n",
       " 'els': 881,\n",
       " 'play': 882,\n",
       " 'newest': 883,\n",
       " 'gamejam': 884,\n",
       " 'irish': 885,\n",
       " 'literatur': 886,\n",
       " 'inaccess': 887,\n",
       " \"kareena'\": 888,\n",
       " 'fan': 889,\n",
       " 'brain': 890,\n",
       " 'dot': 891,\n",
       " 'braindot': 892,\n",
       " 'fair': 893,\n",
       " 'rush': 894,\n",
       " 'either': 895,\n",
       " 'brandi': 896,\n",
       " '18': 897,\n",
       " 'carniv': 898,\n",
       " 'men': 899,\n",
       " 'put': 900,\n",
       " 'mask': 901,\n",
       " 'xavier': 902,\n",
       " 'forneret': 903,\n",
       " 'jennif': 904,\n",
       " 'site': 905,\n",
       " 'free': 906,\n",
       " '50.000': 907,\n",
       " '8': 908,\n",
       " 'ball': 909,\n",
       " 'pool': 910,\n",
       " 'coin': 911,\n",
       " 'edit': 912,\n",
       " 'trish': 913,\n",
       " '‚ô•': 914,\n",
       " 'grate': 915,\n",
       " 'three': 916,\n",
       " 'comment': 917,\n",
       " 'wakeup': 918,\n",
       " 'besid': 919,\n",
       " 'dirti': 920,\n",
       " 'sex': 921,\n",
       " 'lmaooo': 922,\n",
       " 'üò§': 923,\n",
       " 'loui': 924,\n",
       " \"he'\": 925,\n",
       " 'throw': 926,\n",
       " 'caus': 927,\n",
       " 'inspir': 928,\n",
       " 'ff': 929,\n",
       " 'twoof': 930,\n",
       " 'gr8': 931,\n",
       " 'wkend': 932,\n",
       " 'kind': 933,\n",
       " 'exhaust': 934,\n",
       " 'word': 935,\n",
       " 'cheltenham': 936,\n",
       " 'area': 937,\n",
       " 'kale': 938,\n",
       " 'crisp': 939,\n",
       " 'ruin': 940,\n",
       " 'x37': 941,\n",
       " 'open': 942,\n",
       " 'worldwid': 943,\n",
       " 'outta': 944,\n",
       " 'sfvbeta': 945,\n",
       " 'vantast': 946,\n",
       " 'xcylin': 947,\n",
       " 'bundl': 948,\n",
       " 'show': 949,\n",
       " 'internet': 950,\n",
       " 'price': 951,\n",
       " 'realisticli': 952,\n",
       " 'pay': 953,\n",
       " 'net': 954,\n",
       " 'educ': 955,\n",
       " 'power': 956,\n",
       " 'weapon': 957,\n",
       " 'nelson': 958,\n",
       " 'mandela': 959,\n",
       " 'recent': 960,\n",
       " 'j': 961,\n",
       " 'chenab': 962,\n",
       " 'flow': 963,\n",
       " 'pakistan': 964,\n",
       " 'incredibleindia': 965,\n",
       " 'teenchoic': 966,\n",
       " 'choiceinternationalartist': 967,\n",
       " 'superjunior': 968,\n",
       " 'caught': 969,\n",
       " 'first': 970,\n",
       " 'salmon': 971,\n",
       " 'super-blend': 972,\n",
       " 'project': 973,\n",
       " 'youth@bipolaruk.org.uk': 974,\n",
       " 'awesom': 975,\n",
       " 'stream': 976,\n",
       " 'alma': 977,\n",
       " 'mater': 978,\n",
       " 'highschoolday': 979,\n",
       " 'clientvisit': 980,\n",
       " 'faith': 981,\n",
       " 'christian': 982,\n",
       " 'school': 983,\n",
       " 'lizaminnelli': 984,\n",
       " 'upcom': 985,\n",
       " 'uk': 986,\n",
       " 'üòÑ': 987,\n",
       " 'singl': 988,\n",
       " 'hill': 989,\n",
       " 'everi': 990,\n",
       " 'beat': 991,\n",
       " 'wrong': 992,\n",
       " 'readi': 993,\n",
       " 'natur': 994,\n",
       " 'pefumeri': 995,\n",
       " 'workshop': 996,\n",
       " 'neal': 997,\n",
       " 'yard': 998,\n",
       " 'covent': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2d4d1-e68f-47d5-b76b-6c59cb3063d6",
   "metadata": {},
   "source": [
    "### Converting a tweet to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492e7eb3-521a-4293-bdcf-58ecafa4bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    if verbose:\n",
    "        print('List of words from the processed tweet: ')\n",
    "        print(word_l)\n",
    "\n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "\n",
    "     # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.\n",
    "        word_ID = unk_ID if word not in vocab_dict else vocab_dict[word] \n",
    "        \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031d8f59-3a5c-4697-8f69-298bc91d11eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1064, 136, 478, 2351, 744, 8148, 1122, 744, 53, 2, 2671, 790, 2, 2, 348, 600, 2, 3488, 1016, 596, 4558, 9, 1064, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab353479-768d-4eb7-9ca9-db0fdcda52ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function gives bad output for val_pos[1]. Test failed\n",
      "2  Tests passed out of 3\n"
     ]
    }
   ],
   "source": [
    "def test_tweet_to_tensor():\n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [val_pos[1], Vocab],\n",
    "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
    "            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"datatype_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":type([]),\n",
    "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"without_unk_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":6,\n",
    "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
    "        }\n",
    "    ]\n",
    "    count = 0\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"datatype_check\":\n",
    "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"without_unk_check\":\n",
    "                assert None not in tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "    if count == 3:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(count,\" Tests passed out of 3\")\n",
    "test_tweet_to_tensor()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1e233-f97e-4843-930b-106bc7d5d42e",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b64ca5a-e725-4a5b-95a1-ad0eaa884454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of posstive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be even\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - An array specifying the importance of each example\n",
    "        \n",
    "    ''' \n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from pos_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset lenght,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "### END GIVEN CODE ###\n",
    "            \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "    \n",
    "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset length,\n",
    "            if neg_index >= len_data_neg:\n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index = neg_index + 1\n",
    "\n",
    "### END CODE HERE ###        \n",
    "\n",
    "### START GIVEN CODE ###\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Update the start index for positive data \n",
    "        # so that it's n_to_take positions after the current pos_index\n",
    "        pos_index += n_to_take\n",
    "        \n",
    "        # Update the start index for negative data \n",
    "        # so that it's n_to_take positions after the current neg_index\n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "### END GIVEN CODE ###\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = [0] * n_pad\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1] * n_to_take\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of zeros)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = [0] * n_to_take\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### GIVEN CODE ###\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba610129-713c-4270-9cce-ec90ac5d2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5494026f-58b4-42f6-94d2-62d83a245e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[2005 4450 3200    9    0    0    0    0    0    0    0]\n",
      " [4953  566 2000 1453 5173 3498  141 3498  130  458    9]\n",
      " [3760  109  136  582 2929 3968    0    0    0    0    0]\n",
      " [ 249 3760    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "889e5d21-cc4d-41cd-9e37-20a9beb4eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (4, 14)\n",
      "The targets shape is (4,)\n",
      "The example weights shape is (4,)\n",
      "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
      "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
      "input tensor: [5736 2900 3760    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 857  255 3651 5737  306 4457  566 1229 2766  327 1201 3760    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "print(f\"The targets shape is {tmp_targets.shape}\")\n",
    "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed97ab-f481-47cc-81c5-dde3bdb3fbf1",
   "metadata": {},
   "source": [
    "## Training N/W without classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf6a76e2-5a18-448f-b5e6-f0ad00dd827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "vocab_size = len(Vocab)\n",
    "embed_size = 300\n",
    "hidden = 100\n",
    "output_dim = 2\n",
    "init_stdev = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eacf6b57-6a19-4497-95e1-7a0fdffa7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "C  = torch.randn(vocab_size, embed_size)\n",
    "W1 = torch.randn(embed_size, hidden) * init_stdev\n",
    "b1 = torch.zeros(hidden)\n",
    "W2 = torch.randn(hidden, output_dim) * init_stdev\n",
    "b2 = torch.zeros(output_dim)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c0d8db1-dca3-44cc-aedb-86778c2a86f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 537,   48, 2197,    9, 4233,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [  83,  222,   90,    9, 1322,  128,  370,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [  37, 4562, 4563, 4564, 1023, 4565,  109,  993, 1995,  566, 2000,\n",
       "         4566,  566, 3105, 4567,   75,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [3301,  306,    9,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [2049, 3327, 3328, 1293, 3329,    9, 1044, 1098, 3330, 1583, 2035,\n",
       "           92,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 166, 2356,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [  22, 3994,    9,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 272,  355,  356,   15,  357,  212,  330,  109,   75,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 109, 2265, 2676,  993,  908,   44,   73,  286,  225,   49, 2961,\n",
       "         3736, 2265,    9, 2069, 2265, 1512,   92,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 225,    9,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [1380, 1027,   98,  230,   44,  196,  118,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [1570,  566,    9,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [  22, 2653,  134, 2654,  159,   92,   14,   55,  686,   98,    9,\n",
       "           56,   57,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [3953,    9,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 929,  662, 1269,    9,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 353, 5509,  697, 5510, 5511, 4931,  118,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 308, 2641,  308,  699, 1134,  310,  517, 5353, 2044,  699,  308,\n",
       "         3760,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [3029, 3678,  498, 6416,  165,  517,  308,  445,  178, 6416,   44,\n",
       "         3760,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [3564,   44,  790, 1159,  311,  413,   48, 3474, 1792,   21,  710,\n",
       "           63,  134,  219, 3760,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 308,  374, 4783, 3760,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [3878, 5722,   14, 3760,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 254,  311, 6749, 6750, 5747,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [1013, 3407,   73, 2035, 1572, 3760,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [7352, 7352, 7352, 7353, 7353, 7353,   50,  667, 7354, 7355,   95,\n",
       "           14,   95,   14, 7356, 3760, 4261, 7511, 7353, 7353, 7353, 7358,\n",
       "         7359, 7352, 7352, 7352],\n",
       "        [ 413,  857, 3760,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [6126, 2466, 3760,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 763, 1044,  854,  882,  169, 3108, 7291, 8187,   14,  419, 3948,\n",
       "         3760,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 221,  697, 6684, 3760,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 416,  725,  376, 5816, 3760,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 229,   48, 5019, 2560, 3760,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 566,  363, 1360,  773,  363, 1329,   19,  179, 3760,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [5925, 3760,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating embeddings\n",
    "inputs, targets, example_weights = next(train_generator(batch_size, shuffle=True))\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90597809-a5e0-4bb7-9361-ed146e9d8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9682028889656067\n",
      "0.8170320987701416\n",
      "0.7546297311782837\n",
      "0.7334308624267578\n",
      "0.7260501384735107\n",
      "0.7227416038513184\n",
      "0.7205978631973267\n",
      "0.7187909483909607\n",
      "0.7170867323875427\n",
      "0.7154187560081482\n",
      "0.7137678861618042\n",
      "0.7121285796165466\n",
      "0.710499107837677\n",
      "0.7088789939880371\n",
      "0.707267701625824\n",
      "0.7056649327278137\n",
      "0.7040708065032959\n",
      "0.7024849057197571\n",
      "0.7009072303771973\n",
      "0.6993374824523926\n",
      "0.6977755427360535\n",
      "0.6962214112281799\n",
      "0.6946747899055481\n",
      "0.6931354999542236\n",
      "0.6916036605834961\n",
      "0.6900789141654968\n",
      "0.6885610222816467\n",
      "0.6870502233505249\n",
      "0.6855459809303284\n",
      "0.684048593044281\n",
      "0.6825577020645142\n",
      "0.6810732483863831\n",
      "0.6795951128005981\n",
      "0.6781231760978699\n",
      "0.6766573786735535\n",
      "0.6751976013183594\n",
      "0.6737438440322876\n",
      "0.6722958087921143\n",
      "0.6708534955978394\n",
      "0.6694169044494629\n",
      "0.667985737323761\n",
      "0.666560173034668\n",
      "0.6651397943496704\n",
      "0.6637247800827026\n",
      "0.6623148918151855\n",
      "0.6609101891517639\n",
      "0.6595103144645691\n",
      "0.658115565776825\n",
      "0.6567255258560181\n",
      "0.6553401947021484\n",
      "0.6539595127105713\n",
      "0.6525833606719971\n",
      "0.6512118577957153\n",
      "0.6498445272445679\n",
      "0.6484817266464233\n",
      "0.6471230387687683\n",
      "0.6457685828208923\n",
      "0.6444181203842163\n",
      "0.6430715918540955\n",
      "0.6417291164398193\n",
      "0.6403902769088745\n",
      "0.6390552520751953\n",
      "0.6377239227294922\n",
      "0.6363962888717651\n",
      "0.6350721120834351\n",
      "0.6337512135505676\n",
      "0.6324337720870972\n",
      "0.6311196088790894\n",
      "0.6298088431358337\n",
      "0.6285011172294617\n",
      "0.6271964311599731\n",
      "0.6258947849273682\n",
      "0.624596118927002\n",
      "0.6233004927635193\n",
      "0.6220074892044067\n",
      "0.6207173466682434\n",
      "0.619429886341095\n",
      "0.6181450486183167\n",
      "0.616862952709198\n",
      "0.6155833005905151\n",
      "0.6143061518669128\n",
      "0.6130315065383911\n",
      "0.6117592453956604\n",
      "0.6104892492294312\n",
      "0.6092215776443481\n",
      "0.6079562306404114\n",
      "0.6066930294036865\n",
      "0.6054320931434631\n",
      "0.6041732430458069\n",
      "0.602916419506073\n",
      "0.6016616821289062\n",
      "0.6004089117050171\n",
      "0.5991581082344055\n",
      "0.5979093313217163\n",
      "0.5966624617576599\n",
      "0.595417320728302\n",
      "0.5941742062568665\n",
      "0.5929327607154846\n",
      "0.5916930437088013\n",
      "0.5904552340507507\n",
      "0.5892190933227539\n",
      "0.5879846811294556\n",
      "0.5867518782615662\n",
      "0.5855206847190857\n",
      "0.5842911005020142\n",
      "0.5830631852149963\n",
      "0.5818367004394531\n",
      "0.5806119441986084\n",
      "0.5793885588645935\n",
      "0.578166663646698\n",
      "0.5769461989402771\n",
      "0.5757273435592651\n",
      "0.5745099186897278\n",
      "0.5732937455177307\n",
      "0.5720789432525635\n",
      "0.5708655714988708\n",
      "0.5696535706520081\n",
      "0.5684428215026855\n",
      "0.5672333240509033\n",
      "0.5660251975059509\n",
      "0.5648183822631836\n",
      "0.5636126399040222\n",
      "0.5624082684516907\n",
      "0.5612050294876099\n",
      "0.5600029826164246\n",
      "0.5588020086288452\n",
      "0.5576022267341614\n",
      "0.556403636932373\n",
      "0.5552061796188354\n",
      "0.5540096163749695\n",
      "0.5528144240379333\n",
      "0.5516201853752136\n",
      "0.5504269003868103\n",
      "0.5492347478866577\n",
      "0.548043429851532\n",
      "0.546853244304657\n",
      "0.5456639528274536\n",
      "0.544475793838501\n",
      "0.5432883501052856\n",
      "0.5421019792556763\n",
      "0.540916383266449\n",
      "0.5397318601608276\n",
      "0.5385480523109436\n",
      "0.5373652577400208\n",
      "0.5361832976341248\n",
      "0.5350021123886108\n",
      "0.5338217616081238\n",
      "0.5326422452926636\n",
      "0.5314635634422302\n",
      "0.530285656452179\n",
      "0.5291084051132202\n",
      "0.5279320478439331\n",
      "0.5267564654350281\n",
      "0.5255815386772156\n",
      "0.5244073271751404\n",
      "0.5232338905334473\n",
      "0.5220612287521362\n",
      "0.520889163017273\n",
      "0.5197177529335022\n",
      "0.5185470581054688\n",
      "0.5173771381378174\n",
      "0.516207754611969\n",
      "0.5150390863418579\n",
      "0.5138710737228394\n",
      "0.5127036571502686\n",
      "0.5115369558334351\n",
      "0.5103707313537598\n",
      "0.509205162525177\n",
      "0.5080402493476868\n",
      "0.5068758726119995\n",
      "0.5057121515274048\n",
      "0.5045490264892578\n",
      "0.5033864378929138\n",
      "0.5022245049476624\n",
      "0.5010629892349243\n",
      "0.4999021887779236\n",
      "0.49874192476272583\n",
      "0.49758216738700867\n",
      "0.49642297625541687\n",
      "0.4952643811702728\n",
      "0.4941062927246094\n",
      "0.4929488003253937\n",
      "0.49179184436798096\n",
      "0.49063536524772644\n",
      "0.48947957158088684\n",
      "0.48832422494888306\n",
      "0.4871693551540375\n",
      "0.48601508140563965\n",
      "0.4848613440990448\n",
      "0.4837081730365753\n",
      "0.48255547881126404\n",
      "0.4814033508300781\n",
      "0.4802517592906952\n",
      "0.47910070419311523\n",
      "0.47795018553733826\n",
      "0.47680023312568665\n",
      "0.475650817155838\n",
      "0.47450193762779236\n",
      "0.4733535647392273\n",
      "0.4722058176994324\n",
      "0.47105854749679565\n",
      "0.4699118733406067\n",
      "0.4687657356262207\n",
      "0.4676201641559601\n",
      "0.4664752185344696\n",
      "0.46533074975013733\n",
      "0.4641868770122528\n",
      "0.4630435109138489\n",
      "0.46190088987350464\n",
      "0.4607587456703186\n",
      "0.4596172571182251\n",
      "0.45847639441490173\n",
      "0.45733603835105896\n",
      "0.45619627833366394\n",
      "0.4550572335720062\n",
      "0.45391878485679626\n",
      "0.4527810215950012\n",
      "0.45164376497268677\n",
      "0.4505073130130768\n",
      "0.4493713974952698\n",
      "0.44823622703552246\n",
      "0.44710174202919006\n",
      "0.4459678530693054\n",
      "0.44483479857444763\n",
      "0.4437023103237152\n",
      "0.44257062673568726\n",
      "0.44143956899642944\n",
      "0.4403092861175537\n",
      "0.4391798675060272\n",
      "0.4380510747432709\n",
      "0.436923086643219\n",
      "0.4357959032058716\n",
      "0.43466946482658386\n",
      "0.43354395031929016\n",
      "0.43241915106773376\n",
      "0.4312951862812042\n",
      "0.4301721155643463\n",
      "0.42904993891716003\n",
      "0.427928626537323\n",
      "0.4268082082271576\n",
      "0.42568865418434143\n",
      "0.42457008361816406\n",
      "0.42345237731933594\n",
      "0.4223356544971466\n",
      "0.42121994495391846\n",
      "0.4201051592826843\n",
      "0.41899144649505615\n",
      "0.4178787171840668\n",
      "0.41676706075668335\n",
      "0.4156564176082611\n",
      "0.4145469069480896\n",
      "0.41343846917152405\n",
      "0.41233110427856445\n",
      "0.41122496128082275\n",
      "0.410119891166687\n",
      "0.40901610255241394\n",
      "0.4079133868217468\n",
      "0.40681198239326477\n",
      "0.4057117998600006\n",
      "0.40461280941963196\n",
      "0.403515100479126\n",
      "0.40241873264312744\n",
      "0.4013236463069916\n",
      "0.40022987127304077\n",
      "0.39913755655288696\n",
      "0.3980465531349182\n",
      "0.3969569504261017\n",
      "0.395868718624115\n",
      "0.3947819471359253\n",
      "0.39369675517082214\n",
      "0.39261293411254883\n",
      "0.3915306329727173\n",
      "0.3904499113559723\n",
      "0.3893706798553467\n",
      "0.38829299807548523\n",
      "0.3872169554233551\n",
      "0.3861424922943115\n",
      "0.38506966829299927\n",
      "0.38399848341941833\n",
      "0.3829290270805359\n",
      "0.3818611204624176\n",
      "0.38079503178596497\n",
      "0.3797306716442108\n",
      "0.3786679804325104\n",
      "0.3776070475578308\n",
      "0.37654799222946167\n",
      "0.3754907250404358\n",
      "0.374435156583786\n",
      "0.37338149547576904\n",
      "0.3723296523094177\n",
      "0.3712797164916992\n",
      "0.37023165822029114\n",
      "0.3691854476928711\n",
      "0.36814120411872864\n",
      "0.367098867893219\n",
      "0.36605846881866455\n",
      "0.3650199770927429\n",
      "0.36398351192474365\n",
      "0.3629489541053772\n",
      "0.3619164824485779\n",
      "0.3608860671520233\n",
      "0.3598575294017792\n",
      "0.35883110761642456\n",
      "0.3578066825866699\n",
      "0.35678431391716003\n",
      "0.3557640314102173\n",
      "0.3547457754611969\n",
      "0.35372963547706604\n",
      "0.3527156114578247\n",
      "0.3517036736011505\n",
      "0.3506937623023987\n",
      "0.34968602657318115\n",
      "0.34868040680885315\n",
      "0.34767693281173706\n",
      "0.3466756045818329\n",
      "0.34567639231681824\n",
      "0.34467941522598267\n",
      "0.3436845541000366\n",
      "0.3426918089389801\n",
      "0.3417012393474579\n",
      "0.34071287512779236\n",
      "0.33972668647766113\n",
      "0.338742733001709\n",
      "0.33776092529296875\n",
      "0.3367813527584076\n",
      "0.33580395579338074\n",
      "0.3348286747932434\n",
      "0.3338557183742523\n",
      "0.33288493752479553\n",
      "0.33191636204719543\n",
      "0.3309500813484192\n",
      "0.32998600602149963\n",
      "0.32902413606643677\n",
      "0.32806453108787537\n",
      "0.32710713148117065\n",
      "0.3261520266532898\n",
      "0.3251991271972656\n",
      "0.32424846291542053\n",
      "0.3233000934123993\n",
      "0.32235392928123474\n",
      "0.32141005992889404\n",
      "0.32046839594841003\n",
      "0.31952908635139465\n",
      "0.31859198212623596\n",
      "0.31765708327293396\n",
      "0.31672459840774536\n",
      "0.3157943785190582\n",
      "0.31486642360687256\n",
      "0.3139406740665436\n",
      "0.3130173087120056\n",
      "0.31209608912467957\n",
      "0.3111772835254669\n",
      "0.31026071310043335\n",
      "0.30934643745422363\n",
      "0.30843448638916016\n",
      "0.3075248599052429\n",
      "0.30661752820014954\n",
      "0.3057124614715576\n",
      "0.30480971932411194\n",
      "0.3039093017578125\n",
      "0.30301111936569214\n",
      "0.3021153509616852\n",
      "0.30122190713882446\n",
      "0.3003306984901428\n",
      "0.2994418740272522\n",
      "0.2985553741455078\n",
      "0.2976711690425873\n",
      "0.296789288520813\n",
      "0.2959097623825073\n",
      "0.2950325012207031\n",
      "0.29415765404701233\n",
      "0.2932851314544678\n",
      "0.2924150228500366\n",
      "0.29154714941978455\n",
      "0.2906816303730011\n",
      "0.2898184657096863\n",
      "0.2889576852321625\n",
      "0.28809916973114014\n",
      "0.2872430682182312\n",
      "0.2863892912864685\n",
      "0.2855379581451416\n",
      "0.28468891978263855\n",
      "0.28384220600128174\n",
      "0.28299790620803833\n",
      "0.2821558713912964\n",
      "0.28131625056266785\n",
      "0.2804790735244751\n",
      "0.2796441316604614\n",
      "0.2788116931915283\n",
      "0.2779814898967743\n",
      "0.27715370059013367\n",
      "0.27632826566696167\n",
      "0.27550530433654785\n",
      "0.2746846377849579\n",
      "0.27386629581451416\n",
      "0.27305036783218384\n",
      "0.27223679423332214\n",
      "0.27142563462257385\n",
      "0.2706168591976166\n",
      "0.26981037855148315\n",
      "0.2690063714981079\n",
      "0.2682046592235565\n",
      "0.26740533113479614\n",
      "0.26660844683647156\n",
      "0.2658138871192932\n",
      "0.26502174139022827\n",
      "0.26423192024230957\n",
      "0.2634444832801819\n",
      "0.2626594305038452\n",
      "0.2618767321109772\n",
      "0.26109638810157776\n",
      "0.26031848788261414\n",
      "0.25954297184944153\n",
      "0.25876978039741516\n",
      "0.2579989731311798\n",
      "0.2572305202484131\n",
      "0.256464421749115\n",
      "0.2557007074356079\n",
      "0.25493937730789185\n",
      "0.2541804015636444\n",
      "0.2534237504005432\n",
      "0.2526695132255554\n",
      "0.2519177198410034\n",
      "0.2511681616306305\n",
      "0.2504209876060486\n",
      "0.2496761977672577\n",
      "0.24893376231193542\n",
      "0.24819359183311462\n",
      "0.2474559247493744\n",
      "0.24672049283981323\n",
      "0.2459874153137207\n",
      "0.24525673687458038\n",
      "0.24452833831310272\n",
      "0.24380235373973846\n",
      "0.24307861924171448\n",
      "0.2423572987318039\n",
      "0.24163824319839478\n",
      "0.2409215122461319\n",
      "0.24020716547966003\n",
      "0.23949512839317322\n",
      "0.23878538608551025\n",
      "0.23807795345783234\n",
      "0.23737286031246185\n",
      "0.2366701066493988\n",
      "0.23596957325935364\n",
      "0.23527143895626068\n",
      "0.2345755249261856\n",
      "0.23388198018074036\n",
      "0.23319070041179657\n",
      "0.23250173032283783\n",
      "0.23181501030921936\n",
      "0.23113055527210236\n",
      "0.23044843971729279\n",
      "0.2297685742378235\n",
      "0.22909094393253326\n",
      "0.2284156233072281\n",
      "0.22774256765842438\n",
      "0.22707176208496094\n",
      "0.22640322148799896\n",
      "0.2257368564605713\n",
      "0.22507284581661224\n",
      "0.22441105544567108\n",
      "0.22375144064426422\n",
      "0.2230941504240036\n",
      "0.22243905067443848\n",
      "0.22178611159324646\n",
      "0.2211354672908783\n",
      "0.22048699855804443\n",
      "0.21984075009822845\n",
      "0.21919679641723633\n",
      "0.21855488419532776\n",
      "0.21791529655456543\n",
      "0.21727782487869263\n",
      "0.2166425883769989\n",
      "0.2160094827413559\n",
      "0.21537859737873077\n",
      "0.21474993228912354\n",
      "0.21412327885627747\n",
      "0.21349892020225525\n",
      "0.21287669241428375\n",
      "0.2122565656900406\n",
      "0.2116386443376541\n",
      "0.21102279424667358\n",
      "0.21040917932987213\n",
      "0.20979763567447662\n",
      "0.20918820798397064\n",
      "0.20858092606067657\n",
      "0.20797578990459442\n",
      "0.20737269520759583\n",
      "0.20677174627780914\n",
      "0.2061728537082672\n",
      "0.2055760622024536\n",
      "0.20498137176036835\n",
      "0.204388827085495\n",
      "0.20379826426506042\n",
      "0.20320986211299896\n",
      "0.20262345671653748\n",
      "0.20203910768032074\n",
      "0.20145685970783234\n",
      "0.20087659358978271\n",
      "0.20029844343662262\n",
      "0.1997222602367401\n",
      "0.19914817810058594\n",
      "0.19857606291770935\n",
      "0.19800597429275513\n",
      "0.19743795692920685\n",
      "0.19687187671661377\n",
      "0.19630785286426544\n",
      "0.19574575126171112\n",
      "0.19518570601940155\n",
      "0.19462765753269196\n",
      "0.19407157599925995\n",
      "0.19351743161678314\n",
      "0.19296526908874512\n",
      "0.1924150586128235\n",
      "0.19186682999134064\n",
      "0.191320538520813\n",
      "0.19077613949775696\n",
      "0.19023379683494568\n",
      "0.18969327211380005\n",
      "0.1891547441482544\n",
      "0.18861807882785797\n",
      "0.18808335065841675\n",
      "0.18755055963993073\n",
      "0.18701966106891632\n",
      "0.18649062514305115\n",
      "0.18596355617046356\n",
      "0.18543829023838043\n",
      "0.18491490185260773\n",
      "0.18439340591430664\n",
      "0.1838737428188324\n",
      "0.18335601687431335\n",
      "0.18284007906913757\n",
      "0.18232597410678864\n",
      "0.1818138062953949\n",
      "0.18130339682102203\n",
      "0.18079480528831482\n",
      "0.18028806149959564\n",
      "0.1797831803560257\n",
      "0.179280087351799\n",
      "0.1787787228822708\n",
      "0.17827923595905304\n",
      "0.17778149247169495\n",
      "0.17728561162948608\n",
      "0.17679142951965332\n",
      "0.17629903554916382\n",
      "0.17580841481685638\n",
      "0.1753196269273758\n",
      "0.17483249306678772\n",
      "0.1743471920490265\n",
      "0.17386355996131897\n",
      "0.1733817458152771\n",
      "0.17290163040161133\n",
      "0.17242321372032166\n",
      "0.17194654047489166\n",
      "0.17147155106067657\n",
      "0.17099830508232117\n",
      "0.17052674293518066\n",
      "0.17005686461925507\n",
      "0.16958875954151154\n",
      "0.16912226378917694\n",
      "0.16865745186805725\n",
      "0.16819430887699127\n",
      "0.1677328199148178\n",
      "0.16727301478385925\n",
      "0.1668148785829544\n",
      "0.16635838150978088\n",
      "0.1659034937620163\n",
      "0.16545026004314423\n",
      "0.16499869525432587\n",
      "0.16454869508743286\n",
      "0.16410034894943237\n",
      "0.16365358233451843\n",
      "0.1632084995508194\n",
      "0.16276496648788452\n",
      "0.1623229831457138\n",
      "0.16188262403011322\n",
      "0.16144387423992157\n",
      "0.16100673377513885\n",
      "0.16057105362415314\n",
      "0.16013704240322113\n",
      "0.1597045212984085\n",
      "0.15927362442016602\n",
      "0.1588442325592041\n",
      "0.15841642022132874\n",
      "0.15799006819725037\n",
      "0.15756531059741974\n",
      "0.1571420133113861\n",
      "0.15672029554843903\n",
      "0.15630008280277252\n",
      "0.1558813899755478\n",
      "0.15546418726444244\n",
      "0.1550484448671341\n",
      "0.15463422238826752\n",
      "0.1542215198278427\n",
      "0.15381023287773132\n",
      "0.1534004658460617\n",
      "0.1529921442270279\n",
      "0.15258529782295227\n",
      "0.15217992663383484\n",
      "0.15177595615386963\n",
      "0.15137343108654022\n",
      "0.1509723961353302\n",
      "0.1505727916955948\n",
      "0.1501745581626892\n",
      "0.1497778594493866\n",
      "0.14938253164291382\n",
      "0.14898855984210968\n",
      "0.14859601855278015\n",
      "0.14820487797260284\n",
      "0.14781518280506134\n",
      "0.14742684364318848\n",
      "0.14703990519046783\n",
      "0.14665426313877106\n",
      "0.14627008140087128\n",
      "0.14588730037212372\n",
      "0.1455058604478836\n",
      "0.14512574672698975\n",
      "0.14474701881408691\n",
      "0.14436960220336914\n",
      "0.1439935714006424\n",
      "0.1436188519001007\n",
      "0.14324548840522766\n",
      "0.14287345111370087\n",
      "0.14250271022319794\n",
      "0.14213328063488007\n",
      "0.14176523685455322\n",
      "0.14139841496944427\n",
      "0.14103291928768158\n",
      "0.14066870510578156\n",
      "0.14030587673187256\n",
      "0.13994424045085907\n",
      "0.13958390057086945\n",
      "0.1392248123884201\n",
      "0.13886705040931702\n",
      "0.1385105550289154\n",
      "0.13815531134605408\n",
      "0.13780130445957184\n",
      "0.13744857907295227\n",
      "0.1370970755815506\n",
      "0.1367468237876892\n",
      "0.1363978087902069\n",
      "0.1360500156879425\n",
      "0.13570347428321838\n",
      "0.13535816967487335\n",
      "0.13501402735710144\n",
      "0.13467109203338623\n",
      "0.1343294233083725\n",
      "0.13398894667625427\n",
      "0.13364963233470917\n",
      "0.13331155478954315\n",
      "0.13297460973262787\n",
      "0.13263891637325287\n",
      "0.1323043555021286\n",
      "0.13197098672389984\n",
      "0.13163883984088898\n",
      "0.13130778074264526\n",
      "0.13097789883613586\n",
      "0.13064919412136078\n",
      "0.13032160699367523\n",
      "0.1299952268600464\n",
      "0.1296699345111847\n",
      "0.12934580445289612\n",
      "0.12902280688285828\n",
      "0.12870094180107117\n",
      "0.1283801645040512\n",
      "0.12806059420108795\n",
      "0.12774202227592468\n",
      "0.12742462754249573\n",
      "0.12710829079151154\n",
      "0.12679316103458405\n",
      "0.12647908926010132\n",
      "0.12616604566574097\n",
      "0.12585416436195374\n",
      "0.12554331123828888\n",
      "0.12523359060287476\n",
      "0.1249249279499054\n",
      "0.1246173158288002\n",
      "0.12431080639362335\n",
      "0.12400535494089127\n",
      "0.12370091676712036\n",
      "0.12339753657579422\n",
      "0.12309527397155762\n",
      "0.1227940171957016\n",
      "0.12249381840229034\n",
      "0.12219464778900146\n",
      "0.12189651280641556\n",
      "0.12159939110279083\n",
      "0.12130329012870789\n",
      "0.12100826948881149\n",
      "0.1207142174243927\n",
      "0.12042117118835449\n",
      "0.12012917548418045\n",
      "0.1198381632566452\n",
      "0.11954814940690994\n",
      "0.11925912648439407\n",
      "0.11897114664316177\n",
      "0.1186840832233429\n",
      "0.1183980256319046\n",
      "0.11811298877000809\n",
      "0.11782889068126678\n",
      "0.11754577606916428\n",
      "0.11726361513137817\n",
      "0.11698246002197266\n",
      "0.11670222878456116\n",
      "0.11642298847436905\n",
      "0.11614467948675156\n",
      "0.11586730927228928\n",
      "0.11559092253446579\n",
      "0.1153154969215393\n",
      "0.11504091322422028\n",
      "0.11476732045412064\n",
      "0.1144947037100792\n",
      "0.11422297358512878\n",
      "0.11395218223333359\n",
      "0.11368228495121002\n",
      "0.11341332644224167\n",
      "0.11314524710178375\n",
      "0.11287812888622284\n",
      "0.11261191219091415\n",
      "0.1123465821146965\n",
      "0.11208215355873108\n",
      "0.11181861907243729\n",
      "0.11155596375465393\n",
      "0.11129418015480042\n",
      "0.11103328317403793\n",
      "0.11077333241701126\n",
      "0.11051420867443085\n",
      "0.11025598645210266\n",
      "0.10999859869480133\n",
      "0.10974210500717163\n",
      "0.10948644578456879\n",
      "0.10923169553279877\n",
      "0.1089777797460556\n",
      "0.1087246686220169\n",
      "0.10847245901823044\n",
      "0.10822106897830963\n",
      "0.10797057300806046\n",
      "0.10772086679935455\n",
      "0.1074720099568367\n",
      "0.10722403228282928\n",
      "0.10697681456804276\n",
      "0.10673043876886368\n",
      "0.10648492723703384\n",
      "0.1062401831150055\n",
      "0.105996273458004\n",
      "0.10575320571660995\n",
      "0.10551093518733978\n",
      "0.1052694320678711\n",
      "0.10502880066633224\n",
      "0.10478891432285309\n",
      "0.10454985499382019\n",
      "0.10431157052516937\n",
      "0.10407409071922302\n",
      "0.10383739322423935\n",
      "0.10360149294137955\n",
      "0.10336633026599884\n",
      "0.1031319871544838\n",
      "0.10289842635393143\n",
      "0.10266561806201935\n",
      "0.10243355482816696\n",
      "0.10220231115818024\n",
      "0.10197180509567261\n",
      "0.10174206644296646\n",
      "0.10151305794715881\n",
      "0.10128481686115265\n",
      "0.10105737298727036\n",
      "0.10083062946796417\n",
      "0.10060463845729828\n",
      "0.10037936270236969\n",
      "0.10015486925840378\n",
      "0.09993112832307816\n",
      "0.09970807284116745\n",
      "0.09948576986789703\n",
      "0.09926418215036392\n",
      "0.0990433320403099\n",
      "0.098823182284832\n",
      "0.09860381484031677\n",
      "0.09838511794805527\n",
      "0.09816709905862808\n",
      "0.09794983267784119\n",
      "0.0977332815527916\n",
      "0.09751741588115692\n",
      "0.09730225056409836\n",
      "0.09708782285451889\n",
      "0.09687406569719315\n",
      "0.09666098654270172\n",
      "0.09644860029220581\n",
      "0.09623690694570541\n",
      "0.09602592140436172\n",
      "0.09581560641527176\n",
      "0.09560593962669373\n",
      "0.0953969955444336\n",
      "0.09518872201442719\n",
      "0.0949811115860939\n",
      "0.09477417171001434\n",
      "0.09456788003444672\n",
      "0.09436225146055222\n",
      "0.09415733069181442\n",
      "0.09395305067300797\n",
      "0.09374939650297165\n",
      "0.09354645013809204\n",
      "0.09334410727024078\n",
      "0.09314242750406265\n",
      "0.09294141829013824\n",
      "0.09274102002382278\n",
      "0.09254129976034164\n",
      "0.09234215319156647\n",
      "0.0921436995267868\n",
      "0.09194586426019669\n",
      "0.0917486697435379\n",
      "0.09155207127332687\n",
      "0.09135613590478897\n",
      "0.09116081148386002\n",
      "0.0909661203622818\n",
      "0.09077201038599014\n",
      "0.09057853370904922\n",
      "0.09038573503494263\n",
      "0.09019345045089722\n",
      "0.09000184386968613\n",
      "0.0898108035326004\n",
      "0.0896204262971878\n",
      "0.08943057805299759\n",
      "0.08924134075641632\n",
      "0.08905274420976639\n",
      "0.08886474370956421\n",
      "0.088677316904068\n",
      "0.08849044144153595\n",
      "0.08830417692661285\n",
      "0.0881185457110405\n",
      "0.08793345093727112\n",
      "0.08774895966053009\n",
      "0.08756507188081741\n",
      "0.08738169819116592\n",
      "0.08719895035028458\n",
      "0.0870167687535286\n",
      "0.08683513849973679\n",
      "0.08665411919355392\n",
      "0.08647361397743225\n",
      "0.08629370480775833\n",
      "0.08611433953046799\n",
      "0.08593551814556122\n",
      "0.0857572928071022\n",
      "0.08557961136102676\n",
      "0.0854024663567543\n",
      "0.08522585779428482\n",
      "0.08504984527826309\n",
      "0.08487436920404434\n",
      "0.08469940721988678\n",
      "0.08452501893043518\n",
      "0.08435117453336716\n",
      "0.08417785912752151\n",
      "0.08400508761405945\n",
      "0.08383282274007797\n",
      "0.08366110920906067\n",
      "0.08348990231752396\n",
      "0.08331925421953201\n",
      "0.08314914256334305\n",
      "0.08297955989837646\n",
      "0.08281046152114868\n",
      "0.0826418548822403\n",
      "0.08247382193803787\n",
      "0.08230629563331604\n",
      "0.0821392834186554\n",
      "0.08197277784347534\n",
      "0.08180677145719528\n",
      "0.08164132386445999\n",
      "0.08147631585597992\n",
      "0.08131184428930283\n",
      "0.08114788681268692\n",
      "0.08098442107439041\n",
      "0.08082142472267151\n",
      "0.08065895736217499\n",
      "0.08049701154232025\n",
      "0.08033549785614014\n",
      "0.08017449826002121\n",
      "0.08001399040222168\n",
      "0.07985396683216095\n",
      "0.07969444245100021\n",
      "0.07953540235757828\n",
      "0.07937681674957275\n",
      "0.07921872287988663\n",
      "0.07906114310026169\n",
      "0.07890400290489197\n",
      "0.07874735444784164\n",
      "0.07859118282794952\n",
      "0.07843543589115143\n",
      "0.07828021049499512\n",
      "0.07812540978193283\n",
      "0.07797110825777054\n",
      "0.07781726121902466\n",
      "0.07766389101743698\n",
      "0.07751098275184631\n",
      "0.07735850661993027\n",
      "0.07720649242401123\n",
      "0.0770549327135086\n",
      "0.07690384238958359\n",
      "0.07675320655107498\n",
      "0.07660301774740219\n",
      "0.07645323127508163\n",
      "0.07630392163991928\n",
      "0.07615507394075394\n",
      "0.076006680727005\n",
      "0.0758587047457695\n",
      "0.07571116089820862\n",
      "0.07556409388780594\n",
      "0.07541744410991669\n",
      "0.07527119666337967\n",
      "0.07512544095516205\n",
      "0.07498007267713547\n",
      "0.0748351514339447\n",
      "0.07469065487384796\n",
      "0.07454659789800644\n",
      "0.07440298795700073\n",
      "0.07425972819328308\n",
      "0.07411690801382065\n",
      "0.07397456467151642\n",
      "0.07383259385824203\n",
      "0.07369109243154526\n",
      "0.07354997098445892\n",
      "0.07340925186872482\n",
      "0.07326894253492355\n",
      "0.0731290727853775\n",
      "0.0729895830154419\n",
      "0.07285051792860031\n",
      "0.07271187007427216\n",
      "0.07257360965013504\n",
      "0.07243575900793076\n",
      "0.07229834049940109\n",
      "0.07216128706932068\n",
      "0.07202465832233429\n",
      "0.07188840210437775\n",
      "0.07175253331661224\n",
      "0.07161708921194077\n",
      "0.07148203253746033\n",
      "0.07134737074375153\n",
      "0.07121311128139496\n",
      "0.07107920199632645\n",
      "0.07094568759202957\n",
      "0.07081260532140732\n",
      "0.07067985832691193\n",
      "0.07054750621318817\n",
      "0.07041555643081665\n",
      "0.07028395682573318\n",
      "0.07015274465084076\n",
      "0.07002194225788116\n",
      "0.06989148259162903\n",
      "0.06976141035556793\n",
      "0.06963168829679489\n",
      "0.06950240582227707\n",
      "0.06937344372272491\n",
      "0.06924484670162201\n",
      "0.06911664456129074\n",
      "0.06898879259824753\n",
      "0.06886131316423416\n",
      "0.06873418390750885\n",
      "0.06860743463039398\n",
      "0.06848101317882538\n",
      "0.0683550164103508\n",
      "0.0682293251156807\n",
      "0.06810397654771805\n",
      "0.06797902286052704\n",
      "0.06785442680120468\n",
      "0.06773019582033157\n",
      "0.06760630756616592\n",
      "0.06748276203870773\n",
      "0.0673595443367958\n",
      "0.06723670661449432\n",
      "0.0671142116189003\n",
      "0.06699208170175552\n",
      "0.06687024235725403\n",
      "0.06674876064062119\n",
      "0.06662765890359879\n",
      "0.06650688499212265\n",
      "0.06638641655445099\n",
      "0.06626635044813156\n",
      "0.06614658236503601\n",
      "0.06602715700864792\n",
      "0.0659080371260643\n",
      "0.06578925997018814\n",
      "0.06567084789276123\n",
      "0.06555274873971939\n",
      "0.06543496996164322\n",
      "0.0653175339102745\n",
      "0.06520044058561325\n",
      "0.06508368253707886\n",
      "0.06496723741292953\n",
      "0.0648510754108429\n",
      "0.0647352784872055\n",
      "0.06461980193853378\n",
      "0.06450463831424713\n",
      "0.06438979506492615\n",
      "0.06427528709173203\n",
      "0.06416106969118118\n",
      "0.06404715776443481\n",
      "0.06393361836671829\n",
      "0.06382035464048386\n",
      "0.0637073963880539\n",
      "0.0635947659611702\n",
      "0.06348243355751038\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    emb = torch.mean(C[inputs], dim=-2)\n",
    "    a1  = torch.tanh(emb @ W1 + b1)\n",
    "    z2  = a1 @ W2 + b2\n",
    "    loss = F.cross_entropy(z2, targets)\n",
    "    print(loss.item())\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42202a3-61a3-40c9-8049-c2342b21b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03636995702981949\n"
     ]
    }
   ],
   "source": [
    "# Validation loss\n",
    "inputs, targets, example_weights = next(val_generator(16, shuffle=True))\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "emb = torch.mean(C[inputs], dim=-2)\n",
    "a1  = torch.tanh(emb @ W1 + b1)\n",
    "z2  = a1 @ W2 + b2\n",
    "loss = F.cross_entropy(z2, targets)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2834a7f-31d1-43d9-bb07-af297951759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Urvish\\AppData\\Local\\Temp\\ipykernel_18856\\369579928.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, example_weights = next(val_generator(100, shuffle=True))\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "emb = torch.mean(C[inputs], dim=-2)\n",
    "a1  = torch.tanh(emb @ W1 + b1)\n",
    "logits  = a1 @ W2 + b2\n",
    "predictions = F.softmax(logits)\n",
    "predictions = predictions.argmax(dim=1)\n",
    "\n",
    "counts = 0\n",
    "for y_prd, y_grd in zip(predictions, targets):\n",
    "    if y_prd == y_grd:\n",
    "        counts += 1\n",
    "print(f'accuracy: {counts/100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e413fbff-09a6-40e7-8e58-fd2575b857d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = torch.tensor(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    inputs = inputs[None, :]\n",
    "    emb = torch.mean(C[inputs], dim=-2)\n",
    "    a1  = torch.tanh(emb @ W1 + b1)\n",
    "    logits  = a1 @ W2 + b2\n",
    "    pred = F.softmax(logits)\n",
    "    print(pred)\n",
    "    pred  = pred.argmax(dim=1)\n",
    "    sentiment = 'negative'\n",
    "    if pred == 1:\n",
    "        sentiment = 'positive'\n",
    "    return pred, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bded44be-f165-4a2b-b659-6de8e9ee5c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.7039]], grad_fn=<SoftmaxBackward0>)\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
      "***\n",
      "is positive.\n",
      "\n",
      "tensor([[0.9551, 0.0449]], grad_fn=<SoftmaxBackward0>)\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"I hated my day, it was the worst, I'm so sad.\"\n",
      "***\n",
      "is negative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Urvish\\AppData\\Local\\Temp\\ipykernel_18856\\1588372461.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "# try a positive sentence\n",
    "sentence = \"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46838a6e-6569-457d-82d2-2856693d4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9580, 0.0420]], grad_fn=<SoftmaxBackward0>)\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"I am sad\"\n",
      "***\n",
      "is negative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Urvish\\AppData\\Local\\Temp\\ipykernel_18856\\1588372461.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "# try a positive sentence\n",
    "sentence = \"I am sad\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463a775-aa1c-45f2-aa65-b94067ed93a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
